{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "from matplotlib import pyplot as plt\n",
    "# ジュピターノートブック上でグラフを表示させるための処理\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ロジスティック回帰分類器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionMH(object):\n",
    "    '''\n",
    "    ロジスティック回帰分類器\n",
    "    \n",
    "    パラメータ\n",
    "    =========\n",
    "    eta : float\n",
    "           学習率（0.0より大きく1.0以下の値）\n",
    "    n_iter : int\n",
    "            トレーニングデータのトレーニング回数\n",
    "    \n",
    "    属性\n",
    "    =========\n",
    "    w_ : 1次元配列\n",
    "            適合後の重み\n",
    "    errors_ : リスト\n",
    "            各エポックでの誤分類数\n",
    "    shuffle : bool （デフォルト : True）\n",
    "            循環を回避するために各エポックでトレーニングデータをシャッフル\n",
    "    random_state : int （デフォルト : None）\n",
    "            シャッフルに使用するランダムステートを設定し、重みを初期化\n",
    "            \n",
    "    '''\n",
    "    def __init__(self, eta=0.01, n_iter=10, shuffle=True, random_state=None):\n",
    "        # 学習の初期化\n",
    "        self.eta = eta\n",
    "        \n",
    "        # トレーニング回数の初期化\n",
    "        self.n_iter = n_iter\n",
    "        \n",
    "        # 重みの初期化フラグはFalseに設定\n",
    "        self.w_initialized = False\n",
    "        \n",
    "        # 各エポックでトレーニングデータをシャッフルするかどうかのフラグを初期化\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        # 引数random_stateが指定された場合は乱数種を設定\n",
    "        if random_state:\n",
    "            seed(random_state)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        トレーニングデータに適合させる\n",
    "        \n",
    "        パラメータ\n",
    "        =========\n",
    "        X : {配列のようなデータ構造}、shape = [n_samples, n_features]\n",
    "                トレーニングデータ\n",
    "        y : 配列のようなデータ構造、shape = [n_samples]\n",
    "                目的変数\n",
    "        \n",
    "        戻り値\n",
    "        =========\n",
    "        self : object\n",
    "        '''\n",
    "        # 重みベクトルの生成\n",
    "        self.w_ = np.zeros(1 + X.shape[1])\n",
    "        \n",
    "        # コストを格納するリストの生成\n",
    "        self.cost_ = []\n",
    "        \n",
    "        # トレーニング回数分トレーニングデータを反復\n",
    "        for i in range(self.n_iter):\n",
    "            # i番目の時点での予測値(y_val)を取得\n",
    "            y_val = self.activation(X)\n",
    "            \n",
    "            # 誤差を取得\n",
    "            errors = (y - y_val)\n",
    "            \n",
    "            # 対数尤度の最大化\n",
    "            ## wj := wj + ηΣi(yi - φ(zi))*xij (j = 1, ... , m)\n",
    "            neg_grad = X.T.dot(errors) # Σ(yi - φ(zi))*xijに該当する部分\n",
    "            self.w_[1:] += self.eta * neg_grad # インデックス1 ~ mの重み\n",
    "            self.w_[0] += self.eta * errors.sum() # インデックス0の重みs\n",
    "            \n",
    "            # エポックごとのトレーニングサンプルをすべて分類するコスト\n",
    "            ## 個々のトレーニングサンプルを評価した後に重みを更新するのではなく、トレーニングデータセット全体を用いて勾配を計算する\n",
    "            self.cost_.append(self._logit_cost(y, y_val))\n",
    "            \n",
    "        return self\n",
    " \n",
    "    def _logit_cost(self, y, y_val):\n",
    "        '''\n",
    "         コスト関数\n",
    "         J(w) = Σ [ - yi*log(φ(zi)) - (1 - yi)*log(1 - φ(zi))\n",
    "        '''\n",
    "        logit = -y.dot(np.log(y_val) - (1 - y).dot(np.log(1 - y_val)))\n",
    "        return logit\n",
    "        \n",
    "    def _sigmoid(self, z):\n",
    "        '''\n",
    "        シグモイド関数\n",
    "        '''\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "        \n",
    "    def net_input(self, X):\n",
    "        '''\n",
    "        総入力を計算\n",
    "        '''\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "  \n",
    "\n",
    "    def activation(self, X):\n",
    "        '''\n",
    "        線形活性化関数の出力を計算\n",
    "        '''\n",
    "        z = self.net_input(X)\n",
    "        return self._sigmoid(z)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.activation(X)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        1ステップ後のクラスラベルを返す\n",
    "        量子化器: 指定した条件を元に2値を返す。\n",
    "        '''\n",
    "        return np.where(self.activation(X) >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Irisデータセットの取得と整形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Irisデータセットをロード\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# 3, 4列目の特徴量を抽出\n",
    "X = iris.data[:, [2, 3]]\n",
    "\n",
    "# クラスラベルを取得\n",
    "y = iris.target\n",
    "\n",
    "# トレーニングデータとテストデータに分割\n",
    "## 全体の30%をテストデータにする\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Irisデータセットを標準化\n",
    "sc = StandardScaler()\n",
    "\n",
    "# トレーニングデータの平均と標準偏差を計算\n",
    "sc.fit(X_train)\n",
    "\n",
    "# 平均と標準偏差を用いて標準化\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ロジスティック回帰分類器にデータを適用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_mh = LogisticRegressionMH(n_iter=15, eta=0.01, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.LogisticRegressionMH at 0x1a20b4fc10>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg_mh.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[457.477139169564,\n",
       " 9507.622352823972,\n",
       " 15992.29291106732,\n",
       " 21755.666911591743,\n",
       " 27215.73329233925,\n",
       " 32522.56964303489,\n",
       " 37745.15196335116,\n",
       " 42919.04852374847,\n",
       " 48063.76724524961,\n",
       " 53190.428778023095,\n",
       " 58305.55930850069,\n",
       " 63413.089157755596,\n",
       " 68515.4472430747,\n",
       " 73614.17563893483,\n",
       " 78710.28134295867]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg_mh.cost_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=1, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# ロジスティック回帰のインスタンスを生成\n",
    "lr = LogisticRegression(random_state=1, multi_class='ovr')\n",
    "\n",
    "# トレーニングデータをモデルに適合させる\n",
    "lr.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 2, 1,\n",
       "       0, 0, 2, 0, 0, 1, 1, 0, 2, 1, 0, 2, 2, 1, 0, 2, 1, 1, 2, 0, 2, 0,\n",
       "       0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1, -1,  1, -1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,\n",
       "        1, -1, -1,  1,  1, -1, -1,  1, -1, -1,  1,  1, -1,  1,  1, -1,  1,\n",
       "        1,  1, -1,  1,  1,  1,  1, -1,  1, -1, -1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg_mh.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
