{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 勾配ブースティング木アルゴリズム"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from mh_tree_algorithm import DecisionTreeMH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. foは、最初の決定木モデル（通常の決定木と同様の方法で生成する）\n",
    "\n",
    "2. m = 1, 2, …, Mとして以下を繰り返す\n",
    "   -  サンプルiごとに損失関数の勾配の計算\n",
    "   - 損失関数の勾配をy(多分目的変数)として、決定木モデルを構築\n",
    "   - 決定木モデルの葉ノードをRmj( jは、葉ノードの番号)とする\n",
    "   - 葉ノードごとに、以下を最小化するmjを計算\n",
    "      - 計算式は、資料を参照する。\n",
    "   - 以下のようにモデルfmを計算\n",
    "     - fm( xi) = fm( xi) + xiが含まれるRmjにおけるmj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostingTree(object):\n",
    "    def __init__(self, n_estimators=100):\n",
    "        self.n_estimators = n_estimators\n",
    "    \n",
    "    def fit(self, X_train. y_train):\n",
    "        f0 = DecisionTreeMH().fit(X_train, y_train)\n",
    "        \n",
    "        for m in range(self.n_estimators):\n",
    "            '''\n",
    "            サンプルiごとに損失関数の勾配の計算\n",
    "            '''\n",
    "            \n",
    "            \n",
    "            '''\n",
    "            損失関数の勾配をy(多分目的変数)として、決定木モデルを構築\n",
    "            '''\n",
    "            '''\n",
    "            決定木モデルの葉ノードをRmj(jは葉ノードの番号)とする\n",
    "            '''\n",
    "            '''\n",
    "            葉ノードごとに、最小化するrmjを計算\n",
    "            '''\n",
    "            '''\n",
    "            モデルfmを計算\n",
    "            '''\n",
    "    \n",
    "    def lossFunction(self, target):\n",
    "        '''\n",
    "        損失関数L(yi, yi^): サンプルごとに、モデルが予測をミスした量を計算する関数\n",
    "          - yiは、サンプルiにおける目的変数\n",
    "          - yi^は、サンプルiにおける予測結果\n",
    "          \n",
    "        交差エントロピー\n",
    "            IH( t ) = - Σi (p(i | t) * log2 p(i | t))、(i = 1..)\n",
    "        '''\n",
    "        classes = np.unique(target)\n",
    "        numdata = len(target)\n",
    "        val = 0\n",
    "            for c in classes:\n",
    "                p = float(len(target[target == c])) / numdata # ノードtarget内でクラスcが含まれる割合を計算 p(i | t)の部分\n",
    "                if p != 0.0:\n",
    "                    val -= p * np.log2(p)\n",
    "        return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
