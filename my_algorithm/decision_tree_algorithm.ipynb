{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 決定木アルゴリズム作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import pydotplus\n",
    "from sklearn import datasets\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ノードクラス(決定木の基となるクラス)の作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class demoNode(object):\n",
    "    def __init__(self, max_depth):\n",
    "        self.left  = None\n",
    "        self.right = None\n",
    "        self.max_depth = max_depth\n",
    "        self.depth = None\n",
    "        \n",
    "    def split_node(self, depth):\n",
    "        self.depth = depth\n",
    "        print (\"Recursion depth: \" + str(self.depth))\n",
    "        \n",
    "        if self.depth == self.max_depth:\n",
    "            return\n",
    "\n",
    "        self.left  = demoNode(self.max_depth)\n",
    "        self.right = demoNode(self.max_depth)\n",
    "\n",
    "        self.left.split_node(depth + 1)   # recursive call\n",
    "        self.right.split_node(depth + 1)  # recursive call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ノードクラスの動作確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 3\n",
    "initial_depth = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recursion depth: 0\n",
      "Recursion depth: 1\n",
      "Recursion depth: 2\n",
      "Recursion depth: 3\n",
      "Recursion depth: 3\n",
      "Recursion depth: 2\n",
      "Recursion depth: 3\n",
      "Recursion depth: 3\n",
      "Recursion depth: 1\n",
      "Recursion depth: 2\n",
      "Recursion depth: 3\n",
      "Recursion depth: 3\n",
      "Recursion depth: 2\n",
      "Recursion depth: 3\n",
      "Recursion depth: 3\n"
     ]
    }
   ],
   "source": [
    "demo_tree = demoNode(max_depth)\n",
    "demo_tree.split_node(initial_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⬆︎再帰呼び出しの呼び出された順番\n",
    "1. ノード0のsplit_node()が呼ばれる。 => 0\n",
    "2. ノード1_1のsplit_node()が呼ばれる。 => 1\n",
    "3. ノード2_1のsplit_node()が呼ばれる。 => 2\n",
    "4. ノード3_1のsplit_node()が呼ばれる。 => 3\n",
    "5. ノード3_2のsplit_node()が呼ばれる。 => 3\n",
    "6. ノード2_2のsplit_node()が呼ばれる。 => 2\n",
    "7. ノード3_3のsplit_node()が呼ばれる。 => 3\n",
    "8. ノード3_4のsplit_node()が呼ばれる。 => 3\n",
    "9. ノード1_2のsplit_node()が呼ばれる。 => 1\n",
    "10. ノード2_3のsplit_node()が呼ばれる。 => 2\n",
    "11. ノード3_5のsplit_node()が呼ばれる。 => 3\n",
    "12. ノード3_6のsplit_node()が呼ばれる。 => 3\n",
    "13. ノード2_4のsplit_node()が呼ばれる。 => 2\n",
    "14. ノード3_7のsplit_node()が呼ばれる。 => 3\n",
    "15. ノード3_8のsplit_node()が呼ばれる。 => 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 決定木で使われる乱数\n",
    "\n",
    "決定木では、各特徴量で情報利得を計算する時に乱数を使用している。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特徴量の重要度\n",
    "\n",
    "決定木では、情報利得が最大になるようにノードを分割していく。\n",
    "\n",
    "情報利得とは、ある特徴量でノードを分割した時に得られる情報量のことである。つまり、情報利得の大きさで分割する特徴量の重要度を考えることができる。\n",
    "\n",
    "反対に重要度が低い特徴量でノードを分割すると、子ノードの不純度は高いままなので学習が中々進まない。\n",
    "\n",
    "加えて、トレーニングサンプルに重要度が低い特徴量がある状態で最後まで決定木のノードを分割した場合、過学習に陥ってしまう可能性も高い。\n",
    "\n",
    "トレーニングサンプルの特徴量数が膨大で学習に時間がかかる場合は、前もって少ないトレーニングサンプルで学習を実施し、各特徴量の重要度を求めておくことで、重要度が低い特徴量をトレーニングサンプルから省くことができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 決定木アルゴリズムの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, criterion=\"gini\", max_depth=None, random_state=None):\n",
    "        self.criterion = criterion # criterion is 基準\n",
    "        self.max_depth = max_depth\n",
    "        self.random_state = random_state\n",
    "        self.depth = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.feature = None # このメンバ変数には、このノードにおける情報利得が最大となる特徴量のインデックス番号が入る。\n",
    "        self.threshold = None # このメンバ変数には、このノードにおける情報利得が最大となるsplit_pointが入る。\n",
    "        self.label = None # このメンバ変数には、このノードにおける属しているデータの個数が最も多いラベル（目的変数）が入る。\n",
    "        self.impurity = None\n",
    "        self.info_gain = None\n",
    "        self.num_samples = None # このノードのデータの個数（サンプル数）\n",
    "        self.num_classes = None\n",
    "        \n",
    "    def split_node(self, sample, target, depth, ini_num_classes):\n",
    "        '''\n",
    "        指定されたmax_depthまでの深さを持つ決定木（ノードツリー）を作成している。\n",
    "        sample: 特徴量{カラム : 学習データ}\n",
    "        target: 目的変数(ラベル)\n",
    "        depth: ノードの深さ\n",
    "        ini_num_classes: 目的変数のラベルの数\n",
    "            （例）二分木ならば、ラベルの数は、0と1で二個になる。\n",
    "        '''\n",
    "        self.depth = depth\n",
    "        self.num_samples = len(target)# このノードのデータの個数（サンプル数）を代入\n",
    "        '''\n",
    "        各ラベルに属するデータの個数を計測してリストで代入\n",
    "       　　 （例）ラベル0のサンプル数100、ラベル1のサンプル数50\n",
    "        '''\n",
    "        self.num_classes = [len(target[target==i]) for i in ini_num_classes]\n",
    "        \n",
    "        if len(np.unique(target)) == 1:# np.unique()は、重複を取り除いた配列を返す。つまりsetを返す。\n",
    "            self.label = target[0] # ラベルが1つしかないのならそもそも分類する必要はない\n",
    "            self.impurity = self.criterion_func(target) # impurity is 不純物\n",
    "            return\n",
    "        \n",
    "        '''\n",
    "        辞書内包表記を使って、ディクショナリ{Key : Value}を作っている。\n",
    "        Key: ラベル、Value: ラベルに属しているデータの個数\n",
    "        　　（例） {ラベル0: 100, ラベル1: 50}\n",
    "        '''\n",
    "        class_count = {i: len(target[target==i]) for i in np.unique(target)}\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        max関数のオプションkeyにlambda構文で作成したディクショナリのValueを返す無名関数を指定する。\n",
    "        そうすることで、max関数はディクショナリのValueが最大になる{Key : Value}のペアを返すようになる。\n",
    "        最終的にlabelには、max関数の戻り値の0番目の要素であるKeyを代入している。\n",
    "        \n",
    "        ちなみにこのlabelは目的変数である。\n",
    "        \n",
    "        今回の場合では、各ノードに含まれるラベルに属するデータの個数が最も多いラベルを返す。\n",
    "        '''\n",
    "        self.label = max(class_count.items(), key=lambda x:x[1])[0]\n",
    "\n",
    "        self.impurity = self.criterion_func(target)# 不純度を計算して代入している。\n",
    "        num_features = sample.shape[1]# 特徴量の数を代入している。\n",
    "        self.info_gain = 0.0# 情報利得の初期化をしている。\n",
    "        \n",
    "        if self.random_state != None:\n",
    "            np.random.seed(self.random_state)\n",
    "        \n",
    "        '''\n",
    "        permutation関数は、引数の配列をランダムに並べ替える。\n",
    "        tolist関数は、配列をリスト型に変換している。\n",
    "        二分木の場合は、[0, 1]か[1, 0]になる。\n",
    "        '''\n",
    "        f_loop_order = np.random.permutation(num_features).tolist()\n",
    "        \n",
    "        '''\n",
    "        特徴量ごとにループを回し、どの特徴量のどの境界(split_point)が\n",
    "        このノードにおける最も高い情報利得を得ることができるかを算出する。\n",
    "        '''\n",
    "        for f in f_loop_order:\n",
    "            uniq_feature = np.unique(sample[:, f]) # 各カラム(特徴量)のデータ一覧を重複を無くして取得している。\n",
    "            \n",
    "            '''\n",
    "            uniq_feature[:-1]は、配列uniq_featureの最後の要素以外のすべての要素を含む配列\n",
    "            uniq_feature[1:]は、配列uniq_featureの最初の要素以外のすべての要素を含む配列\n",
    "              （例） uniq_feature = [1, 2, 3]とする。\n",
    "                          uniq_feature[:-1] = [1, 2]、uniq_feature[1:] = [2, 3]となる。\n",
    "                          よって、uniq_feature[:-1] + [1, 2]、uniq_feature[1:] = [3, 5]\n",
    "                          split_points = (uniq_feature[:-1] + uniq_feature[1:]) / 2.0\n",
    "                                                = [1.5, 2.5]となる。\n",
    "            '''\n",
    "            split_points = (uniq_feature[:-1] + uniq_feature[1:]) / 2.0# 直訳すると split_points is 分割ポイント\n",
    "            \n",
    "            for threshold in split_points:\n",
    "                '''\n",
    "                目的変数リストをleftとrightに分割している。\n",
    "                split_pointによる境界でleftとrightに分けている。\n",
    "                どのsplit_pointによる分割の仕方が最も高い情報利得を得ることができるかを知るために全パターンの算出結果をループで計測している。\n",
    "                ''' \n",
    "                target_l = target[sample[:, f] <= threshold]\n",
    "                target_r= target[sample[:, f] > threshold]\n",
    "                val = self.calc_info_gain(target, target_l, target_r)# 情報利得を算出している。\n",
    "                if self.info_gain < val:# より高い情報利得の場合は、以下の値を更新している。\n",
    "                    self.info_gain = val # このときの情報利得の値を代入\n",
    "                    self.feature = f # この情報利得の値を算出した特徴量を代入\n",
    "                    self.threshold = threshold # この情報利得の値を算出したsplit_pointを代入\n",
    "        \n",
    "        if self.info_gain == 0.0:\n",
    "            return\n",
    "        if self.depth == self.max_depth:\n",
    "            # この条件式を加えることで、max_depthまでの深さの決定木ノードが作られる。\n",
    "            return\n",
    "        \n",
    "        '''\n",
    "        子ノード(左)の設定\n",
    "        このノードにおいて情報利得が最も高くなる特徴量とsplit_pointを使って学習データを設定する。\n",
    "        　　情報利得が最も高くなる特徴量を指定して取得したデータリストがsplit_point以下のデータを設定する。\n",
    "        '''\n",
    "        sample_l = sample[sample[:, self.feature] <= self.threshold]\n",
    "        target_l = target[sample[:, self.feature] <= self.threshold]\n",
    "        self.left = Node(self.criterion, self.max_depth)\n",
    "        self.left.split_node(sample_l, target_l, depth + 1, ini_num_classes) # recursive call\n",
    "        \n",
    "        '''\n",
    "        子ノード(右)の設定\n",
    "        このノードにおいて情報利得が最も高くなる特徴量とsplit_pointを使って学習データを設定する。\n",
    "        　　情報利得が最も高くなる特徴量を指定して取得したデータリストがsplit_pointより大きいデータを設定する。\n",
    "        '''\n",
    "        sample_r = sample[sample[:, self.feature] > self.threshold]\n",
    "        target_r = target[sample[:, self.feature] > self.threshold]\n",
    "        self.right = Node(self.criterion, self.max_depth)\n",
    "        self.right.split_node(sample_r, target_r,  depth + 1, ini_num_classes) # recursive call\n",
    "    \n",
    "    def criterion_func(self, target):\n",
    "        '''\n",
    "        メンバ変数criterionで指定された不純度で不純度を算出する\n",
    "        '''\n",
    "        classes = np.unique(target)\n",
    "        numdata = len(target)\n",
    "        \n",
    "        if self.criterion == \"gini\":\n",
    "            '''\n",
    "            ジニ不純度\n",
    "            IG( t ) = 1 - Σi p(i | t)^2 、(i = 1..)\n",
    "            '''\n",
    "            val = 1\n",
    "            for c in classes:\n",
    "                p = float(len(target[target == c])) / numdata # ノードtarget内でクラスcが含まれる割合を計算 p(i | t)の部分\n",
    "                val -= p ** 2.0 # 1 - Σi p(i | t)^2 、(i = 1..) の部分\n",
    "        elif self.criterion == \"entropy\":\n",
    "            '''\n",
    "            エントロピー\n",
    "            IH( t ) = - Σi (p(i | t) * log2 p(i | t))、(i = 1..)\n",
    "            '''\n",
    "            val = 0\n",
    "            for c in classes:\n",
    "                p = float(len(target[target == c])) / numdata # ノードtarget内でクラスcが含まれる割合を計算 p(i | t)の部分\n",
    "                if p != 0.0:\n",
    "                    val -= p * np.log2(p)\n",
    "        return val\n",
    "            \n",
    "    def calc_info_gain(self, target_p, target_cl, target_cr):\n",
    "        '''\n",
    "        情報利得の算出\n",
    "        　　(親ノードの不純度) - (子ノードの不純度の総和)\n",
    "        IG(Dp, f) = I(Dp) - (Nleft / Np) * I(Dleft) - (Nright / Np) * I(Dright)  \n",
    "        '''\n",
    "        cri_p = self.criterion_func(target_p) # I(Dp)の部分\n",
    "        cri_cl = self.criterion_func(target_cl) # I(Dleft)の部分\n",
    "        cri_cr = self.criterion_func(target_cr) # I(Dright)の部分\n",
    "        return cri_p - len(target_cl) / float(len(target_p)) * cri_cl - len(target_cr) / float(len(target_p)) * cri_cr\n",
    "    \n",
    "    def predict(self, sample):\n",
    "        '''\n",
    "        予測結果ラベル(目的変数)を返す。\n",
    "        ここで使用されているself.featureは、このノードにおいて最も高い情報利得を取得できる特徴量が格納されている。\n",
    "        ここで使用されているself.thresholdは、このノードにおいて最も高い情報利得を取得できるsplit_pointが格納されている。\n",
    "        '''\n",
    "        if self.feature == None or self.depth == self.max_depth:\n",
    "            return self.label\n",
    "        else:\n",
    "            if sample[self.feature] <= self.threshold:\n",
    "                return self.left.predict(sample)\n",
    "            else:\n",
    "                return self.right.predict(sample)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeAnalysis(object):\n",
    "    def __init__(self):\n",
    "        self.num_features = None\n",
    "        self.importances = None\n",
    "    \n",
    "    def compute_feature_importances(self, node):\n",
    "        '''\n",
    "        特徴量の重要度の計算\n",
    "        FI( fi ) = Σt IG(t, fi) * nt、(t ∈ Nfj)\n",
    "        '''\n",
    "        if node.feature == None:\n",
    "            return\n",
    "        \n",
    "        self.importances[node.feature] += node.info_gain * node.num_samples # Σt IG(t, fi) * nt の部分\n",
    "        print(\"分割対象の特徴量\")\n",
    "        print(node.feature)\n",
    "        \n",
    "        '''\n",
    "        総和(Σt)の部分は、再帰で求めている。\n",
    "        各ノードで色々な特徴量が分岐をする条件として使用されている可能性があるため、\n",
    "        指定したノードからリーフノードまで再帰で呼び出し、総和を算出している。\n",
    "        '''\n",
    "        self.compute_feature_importances(node.left)\n",
    "        self.compute_feature_importances(node.right)\n",
    "    \n",
    "    def get_feature_importances(self, node, num_features, normalize=True):\n",
    "        '''\n",
    "        特徴量の重要度の取得\n",
    "        正規化をするかしないかも指定できる。\n",
    "          正規化: FIn( fi ) = FI( fi ) / Σj FI( fi )、(j = 1..n)\n",
    "        '''\n",
    "        self.num_features = num_features\n",
    "        self.importances = np.zeros(num_features) # 初期化: 最初はすべての特徴量の重要度はゼロとなる\n",
    "        \n",
    "        self.compute_feature_importances(node)\n",
    "        \n",
    "        '''\n",
    "        各特徴量の重要度の値をこのノードのデータの個数で割っている。\n",
    "        こうすることで、各特徴量の各ノードにおける重要度を算出する。\n",
    "        '''\n",
    "        self.importances /= node.num_samples \n",
    "        \n",
    "        # 正規化\n",
    "        if normalize:\n",
    "            normalizer = np.sum(self.importances) # Σj FI( fi )の部分\n",
    "            \n",
    "            if normalizer > 0.0:\n",
    "                # ゼロで除算しない\n",
    "                self.importances /= normalizer # FI( fi ) / Σj FI( fi )の部分 \n",
    "        return self.importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    def __init__(self, criterion=\"gini\", max_depth=None, random_state=None):\n",
    "        self.tree = None\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.random_state = random_state\n",
    "        self.tree_analysis = TreeAnalysis()\n",
    "    \n",
    "    def fit(self, sample, target):\n",
    "        '''\n",
    "        sample: 特徴量{カラム : 学習データ}\n",
    "        target: 目的変数(ラベル)\n",
    "        '''\n",
    "        self.tree = Node(self.criterion, self.max_depth, self.random_state)\n",
    "        self.tree.split_node(sample, target, 0, np.unique(target))\n",
    "        self.feature_importances_ = self.tree_analysis.get_feature_importances(self.tree, sample.shape[1])\n",
    "    \n",
    "    def predict(self, sample):\n",
    "        '''\n",
    "        予測結果ラベル(目的変数)の配列を返す。\n",
    "        '''\n",
    "        pred = []\n",
    "        for s in sample:\n",
    "            pred.append(self.tree.predict(s))\n",
    "        return np.array(pred)\n",
    "    \n",
    "    def score(self, sample, target):\n",
    "        return sum(self.predict(sample) == target) / float(len(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 動作確認\n",
    "\n",
    "既存の決定木アルゴリズム（scikit-learn）と自作決定木アルゴリズムの性能比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    iris = datasets.load_iris()\n",
    "    X = iris.data[:,[0,2]]  # sepal length and petal length\n",
    "    y = iris.target\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "    max_depth    = None\n",
    "    random_state = 3\n",
    "\n",
    "    clf_m = DecisionTree(criterion=\"gini\", max_depth=max_depth, random_state=random_state)\n",
    "    clf_m.fit(X_train, y_train)\n",
    "    my_score = clf_m.score(X_test, y_test)\n",
    "\n",
    "    clf_s = tree.DecisionTreeClassifier(criterion=\"gini\", max_depth=max_depth, random_state=random_state)\n",
    "    clf_s.fit(X_train, y_train)\n",
    "    sklearn_score = clf_s.score(X_test ,y_test)\n",
    "    \n",
    "    #--- print score\n",
    "    print(\"-\"*50)\n",
    "    print(\"my decision tree score:\" + str(my_score))\n",
    "    print(\"scikit-learn decision tree score:\" + str(sklearn_score))\n",
    "\n",
    "    #---print feature importances\n",
    "    print(\"-\"*50)\n",
    "    f_importance_m = clf_m.feature_importances_\n",
    "    \n",
    "    f_importance_s = clf_s.feature_importances_\n",
    "\n",
    "    print (\"my decision tree feature importances:\")\n",
    "    for f_name, f_importance in zip(np.array(iris.feature_names)[[0,2]], f_importance_m):\n",
    "        print( \"    \",f_name,\":\", f_importance)\n",
    "\n",
    "    print (\"sklearn decision tree feature importances:\")\n",
    "    for f_name, f_importance in zip(np.array(iris.feature_names)[[0,2]], f_importance_s):\n",
    "        print( \"    \",f_name,\":\", f_importance)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分割対象の特徴量\n",
      "1\n",
      "分割対象の特徴量\n",
      "1\n",
      "分割対象の特徴量\n",
      "1\n",
      "分割対象の特徴量\n",
      "0\n",
      "分割対象の特徴量\n",
      "1\n",
      "分割対象の特徴量\n",
      "0\n",
      "分割対象の特徴量\n",
      "0\n",
      "分割対象の特徴量\n",
      "0\n",
      "分割対象の特徴量\n",
      "1\n",
      "分割対象の特徴量\n",
      "0\n",
      "--------------------------------------------------\n",
      "my decision tree score:0.9333333333333333\n",
      "scikit-learn decision tree score:0.9333333333333333\n",
      "--------------------------------------------------\n",
      "my decision tree feature importances:\n",
      "     sepal length (cm) : 0.05572220815759174\n",
      "     petal length (cm) : 0.9442777918424082\n",
      "sklearn decision tree feature importances:\n",
      "     sepal length (cm) : 0.05572220815759178\n",
      "     petal length (cm) : 0.9442777918424082\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X_mh = iris.data[:,[0,2]]  # sepal length and petal length\n",
    "y_mh = iris.target\n",
    "X_train_mh, X_test_mh, y_train_mh, y_test_mh = train_test_split(X_mh, y_mh, test_size=0.3, random_state=0)\n",
    "\n",
    "max_depth    = None\n",
    "random_state = 3\n",
    "\n",
    "criterion=\"gini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_mh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = y_train_mh\n",
    "sample = X_train_mh\n",
    "ini_num_classes = np.unique(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n"
     ]
    }
   ],
   "source": [
    "num_samples = len(target)\n",
    "print(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, 2, 2, 1, 2, 1, 1, 2, 2, 2, 2, 1, 2, 1, 0, 2, 1, 1, 1, 1,\n",
       "       2, 0, 0, 2, 1, 0, 0, 1, 0, 2, 1, 0, 1, 2, 1, 0, 2, 2, 2, 2, 0, 0,\n",
       "       2, 2, 0, 2, 0, 2, 2, 0, 0, 2, 0, 0, 0, 1, 2, 2, 0, 0, 0, 1, 1, 0,\n",
       "       0, 1, 0, 2, 1, 2, 1, 0, 2, 0, 2, 0, 0, 2, 0, 2, 1, 1, 1, 2, 2, 1,\n",
       "       1, 0, 1, 2, 2, 0, 1, 1, 1, 1, 0, 0, 0, 2, 1, 2, 0])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34, 32, 39]\n"
     ]
    }
   ],
   "source": [
    "num_classes = [len(target[target==i]) for i in ini_num_classes]\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 34, 1: 32, 2: 39}\n"
     ]
    }
   ],
   "source": [
    "class_count = {i: len(target[target==i]) for i in np.unique(target)}\n",
    "print(class_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "num_features = sample.shape[1]\n",
    "print(num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0]\n"
     ]
    }
   ],
   "source": [
    "f_loop_order = np.random.permutation(num_features).tolist()\n",
    "print(f_loop_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1 1.2 1.3 1.4 1.5 1.6 1.7 3.  3.3 3.5 3.6 3.7 3.8 3.9 4.  4.1 4.2 4.3\n",
      " 4.4 4.5 4.6 4.7 4.8 4.9 5.  5.1 5.2 5.3 5.4 5.5 5.6 5.7 5.8 5.9 6.  6.1\n",
      " 6.4 6.6 6.7 6.9]\n"
     ]
    }
   ],
   "source": [
    "f = f_loop_order[0]\n",
    "uniq_feature = np.unique(sample[:, f])\n",
    "print(uniq_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 3. , 3.3, 3.5, 3.6, 3.7, 3.8,\n",
       "       3.9, 4. , 4.1, 4.2, 4.3, 4.4, 4.5, 4.6, 4.7, 4.8, 4.9, 5. , 5.1,\n",
       "       5.2, 5.3, 5.4, 5.5, 5.6, 5.7, 5.8, 5.9, 6. , 6.1, 6.4, 6.6, 6.7])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniq_feature[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 3. , 3.3, 3.5, 3.6, 3.7, 3.8, 3.9,\n",
       "       4. , 4.1, 4.2, 4.3, 4.4, 4.5, 4.6, 4.7, 4.8, 4.9, 5. , 5.1, 5.2,\n",
       "       5.3, 5.4, 5.5, 5.6, 5.7, 5.8, 5.9, 6. , 6.1, 6.4, 6.6, 6.7, 6.9])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniq_feature[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.3,  2.5,  2.7,  2.9,  3.1,  3.3,  4.7,  6.3,  6.8,  7.1,  7.3,\n",
       "        7.5,  7.7,  7.9,  8.1,  8.3,  8.5,  8.7,  8.9,  9.1,  9.3,  9.5,\n",
       "        9.7,  9.9, 10.1, 10.3, 10.5, 10.7, 10.9, 11.1, 11.3, 11.5, 11.7,\n",
       "       11.9, 12.1, 12.5, 13. , 13.3, 13.6])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniq_feature[:-1] + uniq_feature[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.15 1.25 1.35 1.45 1.55 1.65 2.35 3.15 3.4  3.55 3.65 3.75 3.85 3.95\n",
      " 4.05 4.15 4.25 4.35 4.45 4.55 4.65 4.75 4.85 4.95 5.05 5.15 5.25 5.35\n",
      " 5.45 5.55 5.65 5.75 5.85 5.95 6.05 6.25 6.5  6.65 6.8 ]\n"
     ]
    }
   ],
   "source": [
    "split_points = (uniq_feature[:-1] + uniq_feature[1:]) / 2.0\n",
    "print(split_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targetの数: 105\n",
      "target_lの数: 1\n",
      "target_rの数: 104\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 3\n",
      "target_rの数: 102\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 7\n",
      "target_rの数: 98\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 16\n",
      "target_rの数: 89\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 26\n",
      "target_rの数: 79\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 31\n",
      "target_rの数: 74\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 34\n",
      "target_rの数: 71\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 35\n",
      "target_rの数: 70\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 36\n",
      "target_rの数: 69\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 38\n",
      "target_rの数: 67\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 39\n",
      "target_rの数: 66\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 40\n",
      "target_rの数: 65\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 41\n",
      "target_rの数: 64\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 43\n",
      "target_rの数: 62\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 45\n",
      "target_rの数: 60\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 48\n",
      "target_rの数: 57\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 51\n",
      "target_rの数: 54\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 52\n",
      "target_rの数: 53\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 55\n",
      "target_rの数: 50\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 59\n",
      "target_rの数: 46\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 61\n",
      "target_rの数: 44\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 63\n",
      "target_rの数: 42\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 65\n",
      "target_rの数: 40\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 68\n",
      "target_rの数: 37\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 72\n",
      "target_rの数: 33\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 77\n",
      "target_rの数: 28\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 79\n",
      "target_rの数: 26\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 81\n",
      "target_rの数: 24\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 83\n",
      "target_rの数: 22\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 84\n",
      "target_rの数: 21\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 88\n",
      "target_rの数: 17\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 91\n",
      "target_rの数: 14\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 94\n",
      "target_rの数: 11\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 96\n",
      "target_rの数: 9\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 97\n",
      "target_rの数: 8\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 100\n",
      "target_rの数: 5\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 101\n",
      "target_rの数: 4\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 102\n",
      "target_rの数: 3\n",
      "\n",
      "targetの数: 105\n",
      "target_lの数: 104\n",
      "target_rの数: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for threshold in split_points:\n",
    "        target_l = target[sample[:, f] <= threshold] \n",
    "        target_r = target[sample[:, f] >  threshold]\n",
    "        print(\"targetの数: \" + str(len(target)))\n",
    "        print(\"target_lの数: \" + str(len(target_l)))\n",
    "        print(\"target_rの数: \" + str(len(target_r)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-0340c27a3a71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimportances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "importances[1] += 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
