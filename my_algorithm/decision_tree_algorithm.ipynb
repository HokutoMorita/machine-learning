{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 決定木アルゴリズム作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import pydotplus\n",
    "from sklearn import datasets\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ノードクラス(決定木の基となるクラス)の作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class demoNode(object):\n",
    "    def __init__(self, max_depth):\n",
    "        self.left  = None\n",
    "        self.right = None\n",
    "        self.max_depth = max_depth\n",
    "        self.depth = None\n",
    "        \n",
    "    def split_node(self, depth):\n",
    "        self.depth = depth\n",
    "        print (\"Recursion depth: \" + str(self.depth))\n",
    "        \n",
    "        if self.depth == self.max_depth:\n",
    "            return\n",
    "\n",
    "        self.left  = demoNode(self.max_depth)\n",
    "        self.right = demoNode(self.max_depth)\n",
    "\n",
    "        self.left.split_node(depth + 1)   # recursive call\n",
    "        self.right.split_node(depth + 1)  # recursive call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ノードクラスの動作確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 3\n",
    "initial_depth = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recursion depth: 0\n",
      "Recursion depth: 1\n",
      "Recursion depth: 2\n",
      "Recursion depth: 3\n",
      "Recursion depth: 3\n",
      "Recursion depth: 2\n",
      "Recursion depth: 3\n",
      "Recursion depth: 3\n",
      "Recursion depth: 1\n",
      "Recursion depth: 2\n",
      "Recursion depth: 3\n",
      "Recursion depth: 3\n",
      "Recursion depth: 2\n",
      "Recursion depth: 3\n",
      "Recursion depth: 3\n"
     ]
    }
   ],
   "source": [
    "demo_tree = demoNode(max_depth)\n",
    "demo_tree.split_node(initial_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⬆︎再帰呼び出しの呼び出された順番\n",
    "1. ノード0のsplit_node()が呼ばれる。 => 0\n",
    "2. ノード1_1のsplit_node()が呼ばれる。 => 1\n",
    "3. ノード2_1のsplit_node()が呼ばれる。 => 2\n",
    "4. ノード3_1のsplit_node()が呼ばれる。 => 3\n",
    "5. ノード3_2のsplit_node()が呼ばれる。 => 3\n",
    "6. ノード2_2のsplit_node()が呼ばれる。 => 2\n",
    "7. ノード3_3のsplit_node()が呼ばれる。 => 3\n",
    "8. ノード3_4のsplit_node()が呼ばれる。 => 3\n",
    "9. ノード1_2のsplit_node()が呼ばれる。 => 1\n",
    "10. ノード2_3のsplit_node()が呼ばれる。 => 2\n",
    "11. ノード3_5のsplit_node()が呼ばれる。 => 3\n",
    "12. ノード3_6のsplit_node()が呼ばれる。 => 3\n",
    "13. ノード2_4のsplit_node()が呼ばれる。 => 2\n",
    "14. ノード3_7のsplit_node()が呼ばれる。 => 3\n",
    "15. ノード3_8のsplit_node()が呼ばれる。 => 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 決定木で使われる乱数\n",
    "\n",
    "決定木では、各特徴量で情報利得を計算する時に乱数を使用している。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特徴量の重要度\n",
    "\n",
    "決定木では、情報利得が最大になるようにノードを分割していく。\n",
    "\n",
    "情報利得とは、ある特徴量でノードを分割した時に得られる情報量のことである。つまり、情報利得の大きさで分割する特徴量の重要度を考えることができる。\n",
    "\n",
    "反対に重要度が低い特徴量でノードを分割すると、子ノードの不純度は高いままなので学習が中々進まない。\n",
    "\n",
    "加えて、トレーニングサンプルに重要度が低い特徴量がある状態で最後まで決定木のノードを分割した場合、過学習に陥ってしまう可能性も高い。\n",
    "\n",
    "トレーニングサンプルの特徴量数が膨大で学習に時間がかかる場合は、前もって少ないトレーニングサンプルで学習を実施し、各特徴量の重要度を求めておくことで、重要度が低い特徴量をトレーニングサンプルから省くことができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 決定木アルゴリズムの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, criterion=\"gini\", max_depth=None, random_state=None):\n",
    "        self.criterion = criterion # criterion is 基準\n",
    "        self.max_depth = max_depth\n",
    "        self.random_state = random_state\n",
    "        self.depth = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.feature = None\n",
    "        self.threshold = None # threshold is 敷居\n",
    "        self.label = None\n",
    "        self.impurity = None\n",
    "        self.info_gain = None\n",
    "        self.num_samples = None\n",
    "        self.num_classes = None\n",
    "        \n",
    "    def split_node(self, sample, target, depth, ini_num_classes):\n",
    "        self.depth = depth\n",
    "        \n",
    "        self.num_samples = len(target)\n",
    "        self.num_classes = [len(target[target==i]) for i in ini_num_classes]\n",
    "        \n",
    "        # np.unique()は、重複を取り除いた配列を返す。つまりsetを返す。\n",
    "        if len(np.unique(target)) == 1:\n",
    "            self.label = target[0]\n",
    "            self.impurity = self.criterion_func(target) # impurity is 不純物\n",
    "            return\n",
    "        \n",
    "        # 辞書内包表記を使って、ディクショナリ{Key : Value}を作っている。\n",
    "        class_count = {i: len(target[target==i]) for i in np.unique(target)}\n",
    "        \n",
    "        '''\n",
    "        max関数のオプションkeyにlambda構文で作成したディクショナリのValueを返す無名関数を指定する。\n",
    "        そうすることで、max関数はディクショナリのValueが最大になる{Key : Value}のペアを返すようになる。\n",
    "        最終的にlabelには、max関数の戻り値の0番目の要素であるKeyを代入している。\n",
    "        \n",
    "        ちなみにこのlabelは目的変数である。\n",
    "        '''\n",
    "        self.label = max(class_count.items(), key=lambda x:x[1])[0]\n",
    "        \n",
    "        self.impurity = self.criterion_func(target)\n",
    "        \n",
    "        num_features = sample.shape[1]\n",
    "        self.info_gain = 0.0\n",
    "        \n",
    "        if self.random_state != None:\n",
    "            np.random.seed(self.random_state)\n",
    "        \n",
    "        '''\n",
    "        permutation関数は、引数の配列をランダムに並べ替える。\n",
    "        tolist関数は、配列をリスト型に変換している。\n",
    "        '''\n",
    "        f_loop_order = np.random.permutation(num_features).tolist()\n",
    "        for f in f_loop_order:\n",
    "            # 各カラム(特徴量)のデータを重複を無くして取得している。\n",
    "            uniq_feature = np.unique(sample[:, f])\n",
    "            # 直訳すると split_points is 分割ポイント\n",
    "            split_points = (uniq_feature[:-1] + uniq_feature[1:]) / 2.0\n",
    "            \n",
    "            for threshold in split_points:\n",
    "                # leftとrightに分割している。\n",
    "                target_l = target[sample[:, f] <= threshold]\n",
    "                target_r= target[sample[:, f] > threshold]\n",
    "                \n",
    "                # 情報利得を算出している。\n",
    "                val = self.calc_info_gain(target, target_l, target_r)\n",
    "                \n",
    "                # より高い情報利得の場合は、その情報利得の値とそれを算出した特徴量と敷居を設定している\n",
    "                if self.info_gain < val:\n",
    "                    self.info_gain = val\n",
    "                    self.feature = f\n",
    "                    self.threshold = threshold\n",
    "        \n",
    "        if self.info_gain == 0.0:\n",
    "            return\n",
    "        if self.depth == self.max_depth:\n",
    "            # この条件式を加えることで、max_depthまでの深さの決定木ノードが作られる。\n",
    "            return\n",
    "        \n",
    "        # 子ノード(左)の設定\n",
    "        sample_l = sample[sample[:, self.feature] <= self.threshold]\n",
    "        target_l = target[sample[:, self.feature] <= self.threshold]\n",
    "        self.left = Node(self.criterion, self.max_depth)\n",
    "        self.left.split_node(sample_l, target_l, depth + 1, ini_num_classes) # recursive call\n",
    "        \n",
    "        # 子ノード(右)の設定\n",
    "        sample_r = sample[sample[:, self.feature] > self.threshold]\n",
    "        target_r = target[sample[:, self.feature] > self.threshold]\n",
    "        self.right = Node(self.criterion, self.max_depth)\n",
    "        self.right.split_node(sample_r, target_r,  depth + 1, ini_num_classes) # recursive call\n",
    "    \n",
    "    def criterion_func(self, target):\n",
    "        '''\n",
    "        メンバ変数criterionで指定された不純度で不純度を算出する\n",
    "        '''\n",
    "        classes = np.unique(target)\n",
    "        numdata = len(target)\n",
    "        \n",
    "        if self.criterion == \"gini\":\n",
    "            '''\n",
    "            ジニ不純度\n",
    "            IG( t ) = 1 - Σi p(i | t)^2 、(i = 1..)\n",
    "            '''\n",
    "            val = 1\n",
    "            for c in classes:\n",
    "                p = float(len(target[target == c])) / numdata # ノードtarget内でクラスcが含まれる割合を計算 p(i | t)の部分\n",
    "                val -= p ** 2.0 # 1 - Σi p(i | t)^2 、(i = 1..) の部分\n",
    "        elif self.criterion == \"entropy\":\n",
    "            '''\n",
    "            エントロピー\n",
    "            IH( t ) = - Σi (p(i | t) * log2 p(i | t))、(i = 1..)\n",
    "            '''\n",
    "            val = 0\n",
    "            for c in classes:\n",
    "                p = float(len(target[target == c])) / numdata # ノードtarget内でクラスcが含まれる割合を計算 p(i | t)の部分\n",
    "                if p != 0.0:\n",
    "                    val -= p * np.log2(p)\n",
    "        return val\n",
    "            \n",
    "    def calc_info_gain(self, target_p, target_cl, target_cr):\n",
    "        '''\n",
    "        情報利得の算出\n",
    "        　　(親ノードの不純度) - (子ノードの不純度の総和)\n",
    "        IG(Dp, f) = I(Dp) - (Nleft / Np) * I(Dleft) - (Nright / Np) * I(Dright)  \n",
    "        '''\n",
    "        cri_p = self.criterion_func(target_p) # I(Dp)の部分\n",
    "        cri_cl = self.criterion_func(target_cl) # I(Dleft)の部分\n",
    "        cri_cr = self.criterion_func(target_cr) # I(Dright)の部分\n",
    "        return cri_p - len(target_cl) / float(len(target_p)) * cri_cl - len(target_cr) / float(len(target_p)) * cri_cr\n",
    "    \n",
    "    def predict(self, sample):\n",
    "        '''\n",
    "        予測結果ラベル(目的変数)を返す。\n",
    "        '''\n",
    "        if self.feature == None or self.depth == self.max_depth:\n",
    "            return self.label\n",
    "        else:\n",
    "            if sample[self.feature] <= self.threshold:\n",
    "                return self.left.predict(sample)\n",
    "            else:\n",
    "                return self.right.predict(sample)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeAnalysis(object):\n",
    "    def __init__(self):\n",
    "        self.num_features = None\n",
    "        self.importances = None\n",
    "    \n",
    "    def compute_feature_importances(self, node):\n",
    "        '''\n",
    "        特徴量の重要度の計算\n",
    "        FI( fi ) = Σt IG(t, fi) * nt、(t ∈ Nfj)\n",
    "        '''\n",
    "        if node.feature == None:\n",
    "            return\n",
    "        \n",
    "        self.importances[node.feature] += node.info_gain * node.num_samples # Σt IG(t, fi) * nt の部分\n",
    "        \n",
    "        # 総和(Σt)の部分は、再帰で求めている。\n",
    "        self.compute_feature_importances(node.left)\n",
    "        self.compute_feature_importances(node.right)\n",
    "    \n",
    "    def get_feature_importances(self, node, num_features, normalize=True):\n",
    "        '''\n",
    "        特徴量の重要度の取得\n",
    "        正規化をするかしないかも指定できる。\n",
    "          正規化: FIn( fi ) = FI( fi ) / Σj FI( fi )、(j = 1..n)\n",
    "        '''\n",
    "        self.num_features = num_features\n",
    "        self.importances = np.zeros(num_features) # 初期化\n",
    "        \n",
    "        self.compute_feature_importances(node)\n",
    "        self.importances /= node.num_samples #??? 重要度の値を小さくしている？？？正規化で値を小さくするために？？？ \n",
    "        \n",
    "        if normalize:\n",
    "            normalizer = np.sum(self.importances) # Σj FI( fi )の部分\n",
    "            \n",
    "            if normalizer > 0.0:\n",
    "                # ゼロで除算しない\n",
    "                self.importances /= normalizer # FI( fi ) / Σj FI( fi )の部分 \n",
    "        return self.importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    def __init__(self, criterion=\"gini\", max_depth=None, random_state=None):\n",
    "        self.tree = None\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.random_state = random_state\n",
    "        self.tree_analysis = TreeAnalysis()\n",
    "    \n",
    "    def fit(self, sample, target):\n",
    "        self.tree = Node(self.criterion, self.max_depth, self.random_state)\n",
    "        self.tree.split_node(sample, target, 0, np.unique(target))\n",
    "        self.feature_importances_ = self.tree_analysis.get_feature_importances(self.tree, sample.shape[1])\n",
    "    \n",
    "    def predict(self, sample):\n",
    "        '''\n",
    "        予測結果ラベル(目的変数)の配列を返す。\n",
    "        '''\n",
    "        pred = []\n",
    "        for s in sample:\n",
    "            pred.append(self.tree.predict(s))\n",
    "        return np.array(pred)\n",
    "    \n",
    "    def score(self, sample, target):\n",
    "        return sum(self.predict(sample) == target) / float(len(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeStructure(object):\n",
    "    def __init__(self):\n",
    "        self.num_node = None\n",
    "        self.dot_data = None\n",
    "        \n",
    "    def print_tree(self, node, feature_names, class_names, parent_node_num):\n",
    "        node.my_node_num = self.num_node\n",
    "        node.parent_node_num = parent_node_num\n",
    "\n",
    "        tree_str = \"\"\n",
    "        if node.feature == None or node.depth == node.max_depth:\n",
    "            tree_str += str(self.num_node) + \" [label=<\" + node.criterion + \" = \" + \"%.4f\" % (node.impurity) + \"<br/>\" \\\n",
    "                                           + \"samples = \" + str(node.num_samples) + \"<br/>\" \\\n",
    "                                           + \"value = \" + str(node.num_classes) + \"<br/>\" \\\n",
    "                                           + \"class = \" + class_names[node.label] + \">, fillcolor=\\\"#00000000\\\"] ;\\n\"\n",
    "            if node.my_node_num!=node.parent_node_num:\n",
    "                tree_str += str(node.parent_node_num) + \" -> \" \n",
    "                tree_str += str(node.my_node_num)\n",
    "                if node.parent_node_num==0 and node.my_node_num==1:\n",
    "                    tree_str += \" [labeldistance=2.5, labelangle=45, headlabel=\\\"True\\\"] ;\\n\"\n",
    "                elif node.parent_node_num==0:\n",
    "                    tree_str += \" [labeldistance=2.5, labelangle=-45, headlabel=\\\"False\\\"] ;\\n\"\n",
    "                else:\n",
    "                    tree_str += \" ;\\n\"\n",
    "            self.dot_data += tree_str\n",
    "        else:\n",
    "            tree_str += str(self.num_node) + \" [label=<\" + feature_names[node.feature] + \" &le; \" + str(node.threshold) + \"<br/>\" \\\n",
    "                                           + node.criterion + \" = \" + \"%.4f\" % (node.impurity) + \"<br/>\" \\\n",
    "                                           + \"samples = \" + str(node.num_samples) + \"<br/>\" \\\n",
    "                                           + \"value = \" + str(node.num_classes) + \"<br/>\" \\\n",
    "                                           + \"class = \" + class_names[node.label] + \">, fillcolor=\\\"#00000000\\\"] ;\\n\"\n",
    "            if node.my_node_num!=node.parent_node_num:\n",
    "                tree_str += str(node.parent_node_num) + \" -> \" \n",
    "                tree_str += str(node.my_node_num)\n",
    "                if node.parent_node_num==0 and node.my_node_num==1:\n",
    "                    tree_str += \" [labeldistance=2.5, labelangle=45, headlabel=\\\"True\\\"] ;\\n\"\n",
    "                elif node.parent_node_num==0:\n",
    "                    tree_str += \" [labeldistance=2.5, labelangle=-45, headlabel=\\\"False\\\"] ;\\n\"\n",
    "                else:\n",
    "                    tree_str += \" ;\\n\"\n",
    "            self.dot_data += tree_str\n",
    "\n",
    "            self.num_node+=1\n",
    "            self.print_tree(node.left, feature_names, class_names, node.my_node_num)\n",
    "            self.num_node+=1\n",
    "            self.print_tree(node.right, feature_names, class_names, node.my_node_num)\n",
    "\n",
    "    def export_graphviz(self, node, feature_names, class_names):\n",
    "        self.num_node = 0\n",
    "        self.dot_data = \"digraph Tree {\\nnode [shape=box, style=\\\"filled, rounded\\\", color=\\\"black\\\", fontname=helvetica] ;\\nedge [fontname=helvetica] ;\\n\"\n",
    "        self.print_tree(node, feature_names, class_names, 0)\n",
    "        self.dot_data += \"}\"\n",
    "        return self.dot_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 動作確認\n",
    "\n",
    "既存の決定木アルゴリズム（scikit-learn）と自作決定木アルゴリズムの性能比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    iris = datasets.load_iris()\n",
    "    X = iris.data[:,[0,2]]  # sepal length and petal length\n",
    "    y = iris.target\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "    max_depth    = None\n",
    "    random_state = 3\n",
    "\n",
    "    clf_m = DecisionTree(criterion=\"gini\", max_depth=max_depth, random_state=random_state)\n",
    "    clf_m.fit(X_train, y_train)\n",
    "    my_score = clf_m.score(X_test, y_test)\n",
    "\n",
    "    clf_s = tree.DecisionTreeClassifier(criterion=\"gini\", max_depth=max_depth, random_state=random_state)\n",
    "    clf_s.fit(X_train, y_train)\n",
    "    sklearn_score = clf_s.score(X_test ,y_test)\n",
    "    \n",
    "    #--- print score\n",
    "    print(\"-\"*50)\n",
    "    print(\"my decision tree score:\" + str(my_score))\n",
    "    print(\"scikit-learn decision tree score:\" + str(sklearn_score))\n",
    "\n",
    "    #---print feature importances\n",
    "    print(\"-\"*50)\n",
    "    f_importance_m = clf_m.feature_importances_\n",
    "    f_importance_s = clf_s.feature_importances_\n",
    "\n",
    "    print (\"my decision tree feature importances:\")\n",
    "    for f_name, f_importance in zip(np.array(iris.feature_names)[[0,2]], f_importance_m):\n",
    "        print( \"    \",f_name,\":\", f_importance)\n",
    "\n",
    "    print (\"sklearn decision tree feature importances:\")\n",
    "    for f_name, f_importance in zip(np.array(iris.feature_names)[[0,2]], f_importance_s):\n",
    "        print( \"    \",f_name,\":\", f_importance)\n",
    "        \n",
    "    #--- output decision region\n",
    "    #plot_result(clf_m, X_train,y_train, X_test, y_test, \"my_decision_tree\")\n",
    "    #plot_result(clf_s, X_train,y_train, X_test, y_test, \"sklearn_decision_tree\")\n",
    "    \n",
    "    #---output decision tree chart\n",
    "  #  tree_ = TreeStructure()\n",
    "   # dot_data_m = tree_.export_graphviz(clf_m.tree, feature_names=np.array(iris.feature_names)[[0,2]], class_names=iris.target_names)\n",
    "    #graph_m = pydotplus.graph_from_dot_data(dot_data_m)\n",
    "\n",
    "    #dot_data_s = tree.export_graphviz(clf_s, out_file=None, feature_names=np.array(iris.feature_names)[[0,2]], class_names=iris.target_names, \n",
    "#                                       filled=True, rounded=True, special_characters=True)  \n",
    "#     graph_s = pydotplus.graph_from_dot_data(dot_data_s)\n",
    "\n",
    "#     graph_m.write_png(\"chart_my_decision_tree.png\")\n",
    "#     graph_s.write_png(\"chart_sklearn_decision_tree.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(clf, X_train,y_train, X_test, y_test, png_name):\n",
    "    X = np.r_[X_train, X_test]\n",
    "    y = np.r_[y_train, y_test]\n",
    "    \n",
    "    markers = ('s','d', 'x','o', '^', 'v')\n",
    "    colors = ('green', 'yellow','red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    labels = ('setosa', 'versicolor', 'virginica')\n",
    "\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    \n",
    "    dx = 0.02\n",
    "    X1 = np.arange(x1_min, x1_max, dx)\n",
    "    X2 = np.arange(x2_min, x2_max, dx)\n",
    "    X1, X2 = np.meshgrid(X1, X2)\n",
    "    Z = clf.predict(np.array([X1.ravel(), X2.ravel()]).T)\n",
    "    Z = Z.reshape(X1.shape)\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.clf()\n",
    "    plt.contourf(X1, X2, Z, alpha=0.5, cmap=cmap)\n",
    "    plt.xlim(X1.min(), X1.max())\n",
    "    plt.ylim(X2.min(), X2.max())\n",
    "    \n",
    "    for idx, cl in enumerate(np.unique(y_train)):\n",
    "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
    "                    alpha=1.0, c=cmap(idx),\n",
    "                    marker=markers[idx], label=labels[idx])\n",
    "        \n",
    "    plt.scatter(x=X_test[:, 0], y=X_test[:, 1], c=\"\", marker=\"o\", s=100,  label=\"test set\")\n",
    "\n",
    "    plt.title(\"Decision region(\" + png_name + \")\")\n",
    "    plt.xlabel(\"Sepal length [cm]\")\n",
    "    plt.ylabel(\"Petal length [cm]\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.grid()\n",
    "    #--plt.show()\n",
    "    plt.savefig(\"decision_region_\" + png_name + \".png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "my decision tree score:0.9333333333333333\n",
      "scikit-learn decision tree score:0.9333333333333333\n",
      "--------------------------------------------------\n",
      "my decision tree feature importances:\n",
      "     sepal length (cm) : 0.05572220815759174\n",
      "     petal length (cm) : 0.9442777918424082\n",
      "sklearn decision tree feature importances:\n",
      "     sepal length (cm) : 0.05572220815759178\n",
      "     petal length (cm) : 0.9442777918424082\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
