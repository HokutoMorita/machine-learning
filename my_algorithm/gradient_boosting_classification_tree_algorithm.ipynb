{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 勾配ブースティング木アルゴリズム"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from mh_tree_algorithm import DecisionTreeMH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. foは、最初の決定木モデル（通常の決定木と同様の方法で生成する）\n",
    "\n",
    "2. m = 1, 2, …, Mとして以下を繰り返す\n",
    "   -  サンプルiごとに損失関数の勾配の計算\n",
    "   - 損失関数の勾配をy(多分目的変数)として、決定木モデルを構築\n",
    "   - 決定木モデルの葉ノードをRmj( jは、葉ノードの番号)とする\n",
    "   - 葉ノードごとに、以下を最小化するmjを計算\n",
    "      - 計算式は、資料を参照する。\n",
    "   - 以下のようにモデルfmを計算\n",
    "     - fm( xi) = fm( xi) + xiが含まれるRmjにおけるmj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テストデータ生成用関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(N=1000, test_size=0.2, seed=123):\n",
    "    np.random.seed(seed)\n",
    "    X = np.linspace(0, 2 * np.pi, N)\n",
    "    X = X.reshape(-1, 1)\n",
    "    y = 10 * np.sin(X[:, 0]) + np.random.standard_normal(N)\n",
    "     \n",
    "    return train_test_split(X, y, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 決定木\n",
    "GBDTを構成する弱学習器である決定木あるいは回帰木は、情報利得が最大となる特徴でデータを再帰的に分割するアルゴリズムである。\n",
    "\n",
    "通常は、二分決定木となる。分割条件は、分類問題の場合はエントロピー、ジニ不純度、分類誤差などで、回帰問題の場合は、MSE(mean squared error)、LAD(least absolute deviation)などがある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treeクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree(object):\n",
    "    def __init__(self, pre_pruning=False, max_depth=6):\n",
    "        self.feature = None\n",
    "        self.label = None\n",
    "        self.n_samples = None\n",
    "        self.gain = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.threshold = None\n",
    "        self.pre_pruning = pre_pruning\n",
    "        self.max_depth = max_depth\n",
    "        self.depth = 0\n",
    " \n",
    "    def build(self, features, target, criterion='gini'):\n",
    "        self.n_samples = features.shape[0]\n",
    " \n",
    "        if len(np.unique(target)) == 1:\n",
    "            self.label = target[0]\n",
    "            return\n",
    " \n",
    "        best_gain = 0.0\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    " \n",
    "        if criterion in {'gini', 'entropy', 'error'}:\n",
    "            self.label = max(target, key=lambda c: len(target[target==c]))\n",
    "        else:\n",
    "            self.label = np.mean(target)\n",
    " \n",
    "        impurity_node = self._calc_impurity(criterion, target)\n",
    " \n",
    "        for col in range(features.shape[1]):\n",
    "            feature_level = np.unique(features[:,col])\n",
    "            thresholds = (feature_level[:-1] + feature_level[1:]) / 2.0\n",
    " \n",
    "            for threshold in thresholds:\n",
    "                target_l = target[features[:,col] <= threshold]\n",
    "                impurity_l = self._calc_impurity(criterion, target_l)\n",
    "                n_l = target_l.shape[0] / self.n_samples\n",
    " \n",
    "                target_r = target[features[:,col] > threshold]\n",
    "                impurity_r = self._calc_impurity(criterion, target_r)\n",
    "                n_r = target_r.shape[0] / self.n_samples\n",
    " \n",
    "                ig = impurity_node - (n_l * impurity_l + n_r * impurity_r)\n",
    " \n",
    "                if ig > best_gain or best_threshold is None or best_feature is None:\n",
    "                    best_gain = ig\n",
    "                    best_feature = col\n",
    "                    best_threshold = threshold\n",
    " \n",
    "        self.feature = best_feature\n",
    "        self.gain = best_gain\n",
    "        self.threshold = best_threshold\n",
    "        if self.pre_pruning is False or self.depth < self.max_depth:\n",
    "            self._divide_tree(features, target, criterion)\n",
    "        else:\n",
    "            self.feature = None\n",
    " \n",
    "    def _divide_tree(self, features, target, criterion):\n",
    "        features_l = features[features[:, self.feature] <= self.threshold]\n",
    "        target_l = target[features[:, self.feature] <= self.threshold]\n",
    "        self.left = Tree(self.pre_pruning, self.max_depth)\n",
    "        self.left.depth = self.depth + 1\n",
    "        self.left.build(features_l, target_l, criterion)\n",
    " \n",
    "        features_r = features[features[:, self.feature] > self.threshold]\n",
    "        target_r = target[features[:, self.feature] > self.threshold]\n",
    "        self.right = Tree(self.pre_pruning, self.max_depth)\n",
    "        self.right.depth = self.depth + 1\n",
    "        self.right.build(features_r, target_r, criterion)\n",
    " \n",
    " \n",
    "    def _calc_impurity(self, criterion, target):\n",
    "        c = np.unique(target)\n",
    "        s = target.shape[0]\n",
    " \n",
    "        if criterion == 'gini':\n",
    "            return self._gini(target, c, s)\n",
    "        elif criterion == 'entropy':\n",
    "            return self._entropy(target, c, s)\n",
    "        elif criterion == 'error':\n",
    "            return self._classification_error(target, c, s)\n",
    "        elif criterion == 'mse':\n",
    "            return self._mse(target)\n",
    "        else:\n",
    "            return self._gini(target, c, s)\n",
    " \n",
    "    def _gini(self, target, classes, n_samples):\n",
    "        gini_index = 1.0\n",
    "        gini_index -= sum([(len(target[target==c]) / n_samples) ** 2 for c in classes])\n",
    "        return gini_index\n",
    " \n",
    "    def _entropy(self, target, classes, n_samples):\n",
    "        entropy = 0.0\n",
    "        for c in classes:\n",
    "            p = len(target[target==c]) / n_samples\n",
    "            if p > 0.0:\n",
    "                entropy -= p * np.log2(p)\n",
    "        return entropy\n",
    " \n",
    "    def _classification_error(self, target, classes, n_samples):\n",
    "        return 1.0 - max([len(target[target==c]) / n_samples for c in classes])\n",
    " \n",
    "    def _mse(self, target):\n",
    "        y_hat = np.mean(target)\n",
    "        return np.square(target - y_hat).mean()\n",
    " \n",
    "    # 決定木の事後剪定\n",
    "    def prune(self, method, max_depth, min_criterion, n_samples):\n",
    "        if self.feature is None:\n",
    "            return\n",
    " \n",
    "        self.left.prune(method, max_depth, min_criterion, n_samples)\n",
    "        self.right.prune(method, max_depth, min_criterion, n_samples)\n",
    " \n",
    "        pruning = False\n",
    " \n",
    "        if method == 'impurity' and self.left.feature is None and self.right.feature is None: # Leaf\n",
    "            if (self.gain * self.n_samples / n_samples) < min_criterion:\n",
    "                pruning = True\n",
    "        elif method == 'depth' and self.depth >= max_depth:\n",
    "            pruning = True\n",
    " \n",
    "        if pruning is True:\n",
    "            self.left = None\n",
    "            self.right = None\n",
    "            self.feature = None\n",
    " \n",
    "    def predict(self, d):\n",
    "        if self.feature is None: # Leaf\n",
    "            return self.label\n",
    "        else: # Node\n",
    "            if d[self.feature] <= self.threshold:\n",
    "                return self.left.predict(d)\n",
    "            else:\n",
    "                return self.right.predict(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回帰木"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor(object):\n",
    "    def __init__(self, criterion='mse', pre_pruning=False, pruning_method='depth', max_depth=3, min_criterion=0.05):\n",
    "        self.root = None\n",
    "        self.criterion = criterion\n",
    "        self.pre_pruning = pre_pruning\n",
    "        self.pruning_method = pruning_method\n",
    "        self.max_depth = max_depth\n",
    "        self.min_criterion = min_criterion\n",
    " \n",
    "    def fit(self, features, target):\n",
    "        self.root = Tree(self.pre_pruning, self.max_depth)\n",
    "        self.root.build(features, target, self.criterion)\n",
    "        if self.pre_pruning is False: # post-pruning\n",
    "            self.root.prune(self.pruning_method, self.max_depth, self.min_criterion, self.root.n_samples)\n",
    " \n",
    "    def predict(self, features):\n",
    "        return np.array([self.root.predict(f) for f in features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 動作確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = generate_data()\n",
    "regressor = DecisionTreeRegressor(criterion='mse', pre_pruning=True, pruning_method='depth', max_depth=3)\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of the Train: 2.92, MSE of the Test: 2.86\n"
     ]
    }
   ],
   "source": [
    "def mse(y, pred):\n",
    "     return np.square(y - pred).mean()\n",
    "\n",
    "print('MSE of the Train: %.2f, MSE of the Test: %.2f' % (mse(y_train, regressor.predict(X_train)), mse(y_test, regressor.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 勾配ブースティング木"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] column: 0, threshold: 3.179330, mse-of-train: 2.92, mse-of-test: 2.84\n",
      "--------------------------------------------------\n",
      "[2] column: 0, threshold: 0.210697, mse-of-train: 2.29, mse-of-test: 2.43\n",
      "--------------------------------------------------\n",
      "[3] column: 0, threshold: 5.430961, mse-of-train: 1.44, mse-of-test: 1.69\n",
      "--------------------------------------------------\n",
      "[4] column: 0, threshold: 0.688697, mse-of-train: 1.20, mse-of-test: 1.50\n",
      "--------------------------------------------------\n",
      "[5] column: 0, threshold: 2.638435, mse-of-train: 1.08, mse-of-test: 1.55\n",
      "--------------------------------------------------\n",
      "[6] column: 0, threshold: 3.915198, mse-of-train: 0.92, mse-of-test: 1.40\n",
      "--------------------------------------------------\n",
      "[7] column: 0, threshold: 6.251738, mse-of-train: 0.89, mse-of-test: 1.41\n",
      "--------------------------------------------------\n",
      "[8] column: 0, threshold: 0.015724, mse-of-train: 0.87, mse-of-test: 1.42\n",
      "--------------------------------------------------\n",
      "[9] column: 0, threshold: 0.952855, mse-of-train: 0.84, mse-of-test: 1.39\n",
      "--------------------------------------------------\n",
      "[10] column: 0, threshold: 5.063027, mse-of-train: 0.81, mse-of-test: 1.39\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = generate_data()\n",
    " \n",
    "M = 10\n",
    "predictions_history = [np.repeat(y_train.mean(), len(y_train))]\n",
    "test_predictions_history = [np.repeat(y_test.mean(), len(y_test))]\n",
    " \n",
    "# LS_TreeBoost (Algorithm-2)\n",
    "for m in range(M):\n",
    "    y_tilde = y_train - predictions_history[-1]\n",
    "    base_learner = DecisionTreeRegressor(criterion='mse', pre_pruning=True, pruning_method='depth', max_depth=3)\n",
    "    base_learner.fit(X_train, y_tilde)\n",
    " \n",
    "    prediction = predictions_history[-1] + base_learner.predict(X_train)\n",
    "    test_prediction = test_predictions_history[-1] + base_learner.predict(X_test)\n",
    " \n",
    "    train_mse = mse(y_train, prediction)\n",
    "    test_mse = mse(y_test, test_prediction)\n",
    "      \n",
    "    predictions_history.append(prediction)\n",
    "    test_predictions_history.append(test_prediction)\n",
    "     \n",
    "    print(\"[%d] column: %d, threshold: %f, mse-of-train: %.2f, mse-of-test: %.2f\\n\" %\n",
    "                  (m+1, base_learner.root.feature, base_learner.root.threshold,\n",
    "                   train_mse, test_mse) + \"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
